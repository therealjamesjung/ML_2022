{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T15:19:24.682947Z","iopub.status.busy":"2022-05-09T15:19:24.682678Z","iopub.status.idle":"2022-05-09T15:19:27.32839Z","shell.execute_reply":"2022-05-09T15:19:27.327339Z","shell.execute_reply.started":"2022-05-09T15:19:24.682921Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","import time\n","import cv2\n","import pickle\n","import torch\n","\n","from sklearn.cluster import KMeans\n","from sklearn.cluster import MiniBatchKMeans\n","from sklearn.svm import SVC\n","from scipy.stats import mode\n","from sklearn.decomposition import PCA\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T15:49:52.349987Z","iopub.status.busy":"2022-05-09T15:49:52.349622Z","iopub.status.idle":"2022-05-09T15:49:52.355414Z","shell.execute_reply":"2022-05-09T15:49:52.354177Z","shell.execute_reply.started":"2022-05-09T15:49:52.349956Z"},"trusted":true},"outputs":[],"source":["# 베이스라인 달성을 위한 파라미터 제공\n","arg_img_size = (128, 128)\n","arg_dense_sift = True\n","args_local_cluster = 200\n","args_global_cluster = 200\n","num_frame = 5\n","\n","#############################\n","# VLAD와 BoW 중 어떤 방식을 사용할 지 결정해줍니다.\n","#############################\n","args_aggr = \"vlad\" # or \"vlad\"\n","\n","pca_vlad = 128"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T15:49:53.733537Z","iopub.status.busy":"2022-05-09T15:49:53.733156Z","iopub.status.idle":"2022-05-09T15:49:53.764993Z","shell.execute_reply":"2022-05-09T15:49:53.764164Z","shell.execute_reply.started":"2022-05-09T15:49:53.733507Z"},"trusted":true},"outputs":[],"source":["# train 비디오의 행동 분류 label read\n","train_csv = pd.read_csv('train_label.csv')\n","train_csv_arr = np.asarray(train_csv)\n","\n","# 데이터 셋에 존재하는 행동 분류 정보 read\n","classinfo = pd.read_csv('class_info.csv')\n","classinfo_arr = np.asarray(classinfo)\n","\n","\n","train_path = os.path.join('train')\n","test_path = os.path.join('test')\n","\n","# train 비디오 경로\n","train_list = os.listdir(train_path)\n","train_list.sort()\n","train_list = [os.path.join(train_path, i) for i in train_list]\n","\n","# test 비디오 경로\n","test_list = os.listdir(test_path)\n","test_list.sort()\n","test_list = [os.path.join(test_path, i) for i in test_list]"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T15:49:54.98436Z","iopub.status.busy":"2022-05-09T15:49:54.983814Z","iopub.status.idle":"2022-05-09T15:49:54.994227Z","shell.execute_reply":"2022-05-09T15:49:54.992927Z","shell.execute_reply.started":"2022-05-09T15:49:54.984316Z"},"trusted":true},"outputs":[],"source":["def video_to_frame(video_path, size, num_frame):\n","    \n","    #########################################################\n","    ## 비디오에서 프레임을 추출해주는 함수\n","    ## \n","    ## Input \n","    ##     video_path : 한 비디오의 경로\n","    ##     size : 비디오 내의 프레임을 읽을 때, 원하는 해상도 크기\n","    ##     num_frames : 한 비디오 내에서 읽을 프레임의 수\n","    ##\n","    ## Output\n","    ##     frames : 읽고 저장한 총 프레임\n","    #########################################################\n","    \n","    cap = cv2.VideoCapture(video_path)\n","    \n","    # 한 비디오의 총 프레임 수 반환 및 원하는 프레임 수 많큼 읽기 위해 읽을 프레임의 인덱스 설정\n","    total_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    sel_ind = np.linspace(0, total_frame-1, num_frame).astype(\"int\")\n","    \n","    \n","    num=0\n","    frames = []\n","    for i in range(total_frame):\n","        \n","        # 읽을 프레임 인덱스의 경우 프레임 읽어 메모리에 저장, 아닐 경우 지나감\n","        if i in sel_ind:\n","            res, frame = cap.read()\n","            # 원하는 해상도로 조절 및 grayscale로 변환\n","            frame = cv2.resize(frame, size, interpolation = cv2.INTER_CUBIC)\n","            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","            frames.append(frame)\n","        else:\n","            res = cap.grab()        \n","    cap.release()\n","    frames = np.asarray(frames)\n","\n","    return frames"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T15:49:55.328688Z","iopub.status.busy":"2022-05-09T15:49:55.328176Z","iopub.status.idle":"2022-05-09T15:49:55.336532Z","shell.execute_reply":"2022-05-09T15:49:55.335428Z","shell.execute_reply.started":"2022-05-09T15:49:55.328655Z"},"trusted":true},"outputs":[],"source":["def computeSIFT(data, dense=False):\n","    \n","    #########################################################\n","    ## 비디오 내의 프레임에서 특징점(visual word)을 SIFT or DenseSIFT로 추출해주는 함수\n","    ## \n","    ## Input \n","    ##     data : 한 비디오에서 읽고 저장한 프레임\n","    ##     dense : SIFT or DenseSIFT 사용 여부\n","    ##\n","    ## Output\n","    ##     x : 프레임에 대해 추출된 특징점(visual word), dict 형태 -> x[0]이면 0번째 인덱스 프레임의 특징점(visual word) [n,128] 확인 가능\n","    #########################################################\n","    \n","    x = {}\n","    for i in range(0, len(data)):\n","        if dense: # DenseSIFT 추출\n","            img = data[i]\n","            step_size = 8\n","            kp = [cv2.KeyPoint(x, y, step_size) for x in range(0, img.shape[0], step_size) for y in range(0, img.shape[1], step_size)]\n","            sift = cv2.SIFT_create()\n","            kp, desc = sift.compute(img, kp)\n","            \n","            ####### Empty Module 1 : DenseSIFT ########\n","            # 기본 SIFT 와 동일하게 정의 \n","            # 기본 SIFT 에서는 detectAndCompute를 사용했지만 \n","            # Dense SIFT는 위에서 생성한 keypoint를 사용해 compute 만을 진행 \n","            #\n","            # 참고) cv2.SIFT_create() 함수와 .compute()함수를 이용하여 구현한다.\n","            ###########################################\n","                       \n","            \n","        \n","        else: # 기본 SIFT 추출\n","            sift = cv2.SIFT_create()\n","            img = data[i]\n","            kp, desc = sift.detectAndCompute(img, None)\n","        x.update({i : desc})\n","\n","    return x"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T15:49:55.433452Z","iopub.status.busy":"2022-05-09T15:49:55.432825Z","iopub.status.idle":"2022-05-09T15:52:28.790394Z","shell.execute_reply":"2022-05-09T15:52:28.789171Z","shell.execute_reply.started":"2022-05-09T15:49:55.433399Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Extract dsift in train data: 100%|██████████| 2020/2020 [00:25<00:00, 78.50it/s]\n","Extract dsift in test data: 100%|██████████| 505/505 [00:06<00:00, 83.73it/s]\n"]}],"source":["# train 비디오에서 프레임 추출 및 특징점(visual word) 추출, dict 형태로 train_local_desc[비디오 경로]이면 해당하는 비디오에서 추출한 모든 특징점(visual word) 확인 가능\n","train_local_desc = {}\n","for vi, vid_path in enumerate(tqdm.tqdm(train_list, desc=\"Extract {} in train data\".format(\"dsift\" if arg_dense_sift else \"sift\"))):\n","    curr_frame = video_to_frame(vid_path, arg_img_size, num_frame)\n","    local_desc = computeSIFT(curr_frame, arg_dense_sift)\n","    train_local_desc.update({vid_path : local_desc})\n","\n","# test 비디오에서 프레임 추출 및 특징점(visual word) 추출, dict 형태로 test_local_desc[비디오 경로]이면 해당하는 비디오에서 추출한 모든 특징점(visual word) 확인 가능\n","test_local_desc = {}\n","for vi, vid_path in enumerate(tqdm.tqdm(test_list, desc=\"Extract {} in test data\".format(\"dsift\" if arg_dense_sift else \"sift\"))):\n","    curr_frame = video_to_frame(vid_path, arg_img_size, num_frame)\n","    local_desc = computeSIFT(curr_frame, arg_dense_sift)\n","    test_local_desc.update({vid_path : local_desc})\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T15:52:28.794049Z","iopub.status.busy":"2022-05-09T15:52:28.793644Z","iopub.status.idle":"2022-05-09T15:52:34.24314Z","shell.execute_reply":"2022-05-09T15:52:34.242212Z","shell.execute_reply.started":"2022-05-09T15:52:28.793993Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","Aggregate SIFT descriptor\n","\t1.17s\n","\n","\n"]}],"source":["print(\"\\n\\nAggregate SIFT descriptor\")\n","start = time.time()\n","\n","\n","# train 비디오 별로 나눠진 특징점(visual word)들을 [n,128]형태로 모음, 모아진 특징점(visual word)들의 정보(비디오 내의 몇번째 프레임에서 나온 특징점인지)는 \n","# 같은 인덱스의 train_frame_total에서 확인 가능 및 비디오 내의 특정 프레임에서 나온 특징점(visual word)의 수는 train_local_info에서 확인 가능\n","train_frame_total = []\n","train_local_desc_total = []\n","train_local_info = {}\n","for k, v in train_local_desc.items():\n","    for kk, vv in v.items():\n","        l_num = 0\n","        if vv is not None:\n","            train_local_desc_total.extend(vv)\n","            train_frame_total.extend([k+\", \"+str(kk)] * len(vv))\n","            l_num = len(vv)\n","        train_local_info.update({k+\", \"+str(kk) : l_num})\n","train_local_desc_total = np.asarray(train_local_desc_total)\n","train_frame_total = np.asarray(train_frame_total)\n","\n","\n","# test 비디오 별로 나눠진 특징점(visual word)들을 [n,128]형태로 모음, 모아진 특징점(visual word)들의 정보(비디오 내의 몇번째 프레임에서 나온 특징점인지)는 \n","# 같은 인덱스의 test_frame_total에서 확인 가능 및 비디오 내의 특정 프레임에서 나온 특징점(visual word)의 수는 test_local_info에서 확인 가능\n","test_frame_total = []\n","test_local_desc_total = []\n","test_local_info = {}\n","for k, v in test_local_desc.items():\n","    for kk, vv in v.items():\n","        l_num = 0\n","        if vv is not None:\n","            test_local_desc_total.extend(vv)\n","            test_frame_total.extend([k+\", \"+str(kk)] * len(vv))\n","            l_num = len(vv)\n","        test_local_info.update({k+\", \"+str(kk) : l_num})\n","test_local_desc_total = np.asarray(test_local_desc_total)\n","test_frame_total = np.asarray(test_frame_total)\n","\n","\n","print(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))"]},{"cell_type":"markdown","metadata":{},"source":["# 비디오 feature 기술을 위한 프레임 feature 기술 (Empty Module 2,3,4)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T15:52:34.246737Z","iopub.status.busy":"2022-05-09T15:52:34.246304Z","iopub.status.idle":"2022-05-09T15:52:34.252872Z","shell.execute_reply":"2022-05-09T15:52:34.251848Z","shell.execute_reply.started":"2022-05-09T15:52:34.246701Z"},"trusted":true},"outputs":[],"source":["def clustering(train_desc, test_desc=None, n_clusters=200):\n","    #########################################################\n","    ## 모든 특징점들 중, 대표 특징점(codebook)을 선정하는 함수\n","    ## \n","    ## Input \n","    ##     train_desc : 모든 train 비디오의 모든 프레임에서 추출한 특징점(visual word)들\n","    ##     test_desc : 모든 test 비디오의 모든 프레임에서 추출한 특징점(visual word)들\n","    ##     n_clusters : 대표 특징점(codebook)의 수\n","    ##\n","    ## Output\n","    ##     train_pred : 대표 특징점(codebook)에 대해 train_desc가 할당된 위치\n","    ##     test_pred : 대표 특징점(codebook)에 대해 train_desc가 할당된 위치\n","    ##     clusters : 대표 특징점(codebook)\n","    ##     kmeans : kmeans 인스턴스\n","    #########################################################\n","    \n","    \n","    ##### Empty Module 2 : 대표 특징점(codebook) 선정 ######\n","    # 제약 조건 : MiniBatchKMeans 사용, random_state=0 고정\n","    # sklearn의 MiniBatchKMeans를 활용하여 n_clusters 크기 만큼의 대표 특징점(codebook)을 선정\n","    ###########################################\n","    \n","\n","    kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=0)\n","    kmeans.fit(train_desc)\n","    train_pred = kmeans.predict(train_desc)\n","    if test_desc is not None:\n","        test_pred = kmeans.predict(test_desc)\n","    else:\n","        test_pred = None\n","    return train_pred, test_pred, kmeans.cluster_centers_, kmeans"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T15:52:34.254706Z","iopub.status.busy":"2022-05-09T15:52:34.254373Z","iopub.status.idle":"2022-05-09T15:55:39.235898Z","shell.execute_reply":"2022-05-09T15:55:39.234866Z","shell.execute_reply.started":"2022-05-09T15:52:34.254678Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["# 모든 train 비디오의 모든 프레임에서 추출한 특징점(visual word)들로 대표 특징점(codebook)을 만들고,\n","# train 비디오의 모든 프레임에서 추출된 특징점(visual word)과 test 비디오의 모든 프레임에서 추출된 특징점(visual word)을 대표 특징점(codebook)에 할당\n","train_local_alloc, test_local_alloc, local_codebook, local_kmeans = clustering(train_local_desc_total, test_local_desc_total, args_local_cluster)"]},{"cell_type":"markdown","metadata":{},"source":["### BoVW (Bag of Visual Word) 및 VLAD (Vector of Locally Aggregated Descriptors)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T15:55:39.240552Z","iopub.status.busy":"2022-05-09T15:55:39.240117Z","iopub.status.idle":"2022-05-09T15:55:39.250173Z","shell.execute_reply":"2022-05-09T15:55:39.249162Z","shell.execute_reply.started":"2022-05-09T15:55:39.240508Z"},"trusted":true},"outputs":[],"source":["def VLAD(X, alloc, centers):\n","    #########################################################\n","    ## 이미지 feature인 VLAD feature를 기술하기 위한 함수\n","    ## \n","    ## Input \n","    ##     X : 한 프레임의 특징점(visual word)들\n","    ##     alloc : 한 프레임의 특징점(visual word)들이 대표 특징점(codebook)에 할당된 위치\n","    ##     centers : 대표 특징점(codebook)\n","    ##\n","    ## Output\n","    ##     V : VLAD feature\n","    #########################################################\n","    \n","    m,d = X.shape\n","    k = centers.shape[0]\n","    \n","    # VLAD feature를 담기 위한 변수\n","    V=np.zeros([k,d])\n","\n","    for i in range(k):\n","        if np.sum(alloc == i) > 0:\n","            V[i] = np.sum(X[alloc == i, :] - centers[i], axis=0 )\n","            ####################### Empty Module 3 : VLAD ########################\n","            # 이미지에서 추출된 특징점(visual word) X와 이들이 대표 특징점(codebook)으로 할당된 정보 alloc을 이용해,\n","            # 동일한 대표 특징점(codebook)으로 할당된 특징점(visual word)들의 벡터 합 계산해 V[i]에 저장\n","            # hint : 바로 위 조건문의 조건 \"alloc==i\"의 의미를 파악하고 인덱싱에 활용\n","            ######################################################################\n","            \n","\n","    \n","    # 후처리 과정\n","    V = V.flatten()\n","    V = np.sign(V)*np.sqrt(np.abs(V))\n","    if np.sqrt(np.dot(V,V))!=0:\n","        V = V/np.sqrt(np.dot(V,V))\n","    return V\n","\n","\n","def BoW(alloc, n_cluster):\n","    #########################################################\n","    ## 이미지 feature인 BoW feature를 기술하기 위한 함수\n","    ## \n","    ## Input \n","    ##     alloc : 한 프레임의 특징점(visual word)들이 대표 특징점(codebook)에 할당된 위치\n","    ##     n_cluster : 대표 특징점(codebook)의 수\n","    ##\n","    ## Output\n","    ##     V : BoW feature\n","    #########################################################\n","    \n","    ######################### Empty Module 4 : BoW ##########################\n","    # 이미지에서 추출된 특징점(visual word)이 대표 특징점(codebook)으로 할당된 정보 alloc의 histogram을 계산\n","    # np.histogram 함수 참고\n","    #########################################################################\n","        \n","    V, _ = np.histogram(alloc, bins=range(n_cluster + 1))\n","    \n","    return V"]},{"cell_type":"markdown","metadata":{},"source":["### 비디오 내 프레임 별로 이미지 feature 기술"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T15:55:39.252983Z","iopub.status.busy":"2022-05-09T15:55:39.252553Z","iopub.status.idle":"2022-05-09T15:57:04.613055Z","shell.execute_reply":"2022-05-09T15:57:04.612279Z","shell.execute_reply.started":"2022-05-09T15:55:39.252931Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","Allocate center & Descript local histogram\n","\t17.81s\n","\n","\n"]}],"source":["print(\"\\n\\nAllocate center & Descript local histogram\")\n","start = time.time()\n","\n","# Train 비디오 내의 프레임 별로 이미지 feature 기술 -> train_global_desc\n","# 각 이미지 feature의 정보(속한 비디오 이름, 비디오 내의 인덱스) -> train_global_desc_key\n","train_global_desc = []\n","train_global_desc_key = []\n","vi=0\n","for k, v in train_local_info.items():\n","    if v!=0:\n","        if args_aggr==\"bow\":            \n","            hist_desc = BoW(train_local_alloc[vi:vi+v], args_local_cluster)\n","        elif args_aggr==\"vlad\":\n","            hist_desc = VLAD(train_local_desc_total[vi:vi+v], train_local_alloc[vi:vi+v], local_codebook)\n","        else:\n","            print(\"Set the appropriate variable args_aggr!!\")\n","            import pdb; pdb.set_trace()\n","\n","        vi+=v\n","        train_global_desc.append(hist_desc)\n","        train_global_desc_key.append(k)\n","train_global_desc = np.asarray(train_global_desc)\n","train_global_desc_key = np.asarray(train_global_desc_key)\n","\n","\n","# Test 비디오 내의 프레임 별로 이미지 feature 기술 -> test_global_desc\n","# 각 이미지 feature의 정보(속한 비디오 이름, 비디오 내의 인덱스) -> test_global_desc_key\n","test_global_desc = []\n","test_global_desc_key = []\n","vi=0\n","for k, v in test_local_info.items():\n","    if v!=0:\n","        if args_aggr==\"bow\":\n","            hist_desc = BoW(test_local_alloc[vi:vi+v], args_local_cluster)\n","        elif args_aggr==\"vlad\":\n","            hist_desc = VLAD(test_local_desc_total[vi:vi+v], test_local_alloc[vi:vi+v], local_codebook)\n","        else:\n","            print(\"Set the appropriate variable args_aggr!!\")\n","            import pdb; pdb.set_trace()\n","\n","        vi+=v\n","        test_global_desc.append(hist_desc)\n","        test_global_desc_key.append(k)\n","test_global_desc = np.asarray(test_global_desc)\n","test_global_desc_key = np.asarray(test_global_desc_key)\n","\n","print(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T15:57:04.615232Z","iopub.status.busy":"2022-05-09T15:57:04.614521Z","iopub.status.idle":"2022-05-09T15:57:39.667236Z","shell.execute_reply":"2022-05-09T15:57:39.665928Z","shell.execute_reply.started":"2022-05-09T15:57:04.615193Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","Reduce dim of descriptor of the frames with PCA\n","\t18.14s\n","\n","\n"]}],"source":["# VLAD feature의 경우 큰 차원으로 인해 메모리 부족 현상이 발생하므로 PCA를 이용한 차원 축소\n","if args_aggr == \"vlad\":\n","    print(\"\\n\\nReduce dim of descriptor of the frames with PCA\")\n","    start = time.time()\n","    pca = PCA(n_components=pca_vlad, random_state=0)\n","    pca.fit(train_global_desc)\n","    train_global_desc = pca.transform(train_global_desc)\n","    test_global_desc = pca.transform(test_global_desc)\n","    print(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T15:57:39.670119Z","iopub.status.busy":"2022-05-09T15:57:39.669248Z","iopub.status.idle":"2022-05-09T15:57:40.790383Z","shell.execute_reply":"2022-05-09T15:57:40.788822Z","shell.execute_reply.started":"2022-05-09T15:57:39.67006Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","Processing label\n","\t0.23s\n","\n","\n"]}],"source":["print(\"\\n\\nProcessing label\")\n","start = time.time()\n","\n","# 분류를 위해, 행동 분류에 대한 train 비디오의 각 프레임 별 label 가공\n","train_global_id = np.array([int(i.split(\"/\")[-1].split(\".\")[0]) for i in train_global_desc_key])\n","train_global_label = []\n","for fid in train_global_id:\n","    cind = np.where(train_csv_arr[:, 0]==fid)[0]\n","    clsname = train_csv_arr[cind, 1]\n","    cinfo_ind = np.where(classinfo_arr[:, 1] == clsname)[0]\n","    train_global_label.append(classinfo_arr[cinfo_ind, 0].astype(\"int\"))\n","train_global_label = np.asarray(train_global_label).ravel()\n","\n","# 분류를 위해, 행동 분류에 대한 test 비디오의 각 프레임 별 id 가공 \n","test_global_id = np.array([int(i.split(\"/\")[-1].split(\".\")[0]) for i in test_global_desc_key])\n","\n","print(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T15:57:40.791996Z","iopub.status.busy":"2022-05-09T15:57:40.791722Z","iopub.status.idle":"2022-05-09T15:57:40.798679Z","shell.execute_reply":"2022-05-09T15:57:40.797855Z","shell.execute_reply.started":"2022-05-09T15:57:40.791968Z"},"trusted":true},"outputs":[],"source":["def saveFile(predict, predict_id, name, best_params=None):\n","    #########################################################\n","    ## 결과를 저장하기 위한 함수\n","    ## \n","    ## Input \n","    ##     predict : 모든 test 비디오의 행동 예측 값\n","    ##     predict_id : 모든 test 비디오의 id\n","    ##     name : 원하는 저장 파일 이름\n","    ##     best_params : 원하는 instance 저장 시 사용\n","    ##\n","    #########################################################\n","    \n","    data = np.concatenate((np.expand_dims(predict_id.astype(\"str\"), axis=1), np.expand_dims(predict.astype(\"str\"), axis=1)), axis=1)\n","    csv = pd.DataFrame(data, columns=['Id', 'Category'])\n","    csv.to_csv(name + \".csv\", index=False)\n","    \n","    if best_params:\n","        f = open(name + \".pickle\", \"wb\")\n","        pickle.dump(best_params, f, 2)"]},{"cell_type":"markdown","metadata":{},"source":["# 비디오의 모든 프레임에서 얻은 feature를 평균내어 비디오 feature 생성 및 분류 (Empty Module 5)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T15:57:40.800051Z","iopub.status.busy":"2022-05-09T15:57:40.799744Z","iopub.status.idle":"2022-05-09T15:57:43.475325Z","shell.execute_reply":"2022-05-09T15:57:43.474335Z","shell.execute_reply.started":"2022-05-09T15:57:40.800004Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","SVM global averaging in frame\n","\t0.56s\n","\n","\n"]}],"source":["print(\"\\n\\nSVM global averaging in frame\")\n","start = time.time()\n","#################### Empty Module 5 : SVM (averaging) ######################\n","# 제약조건 : sklearn의 SVC 사용 및 random_state=0으로 고정\n","#          베이스라인 파라미터는 defalut 값을 이용하였습니다\n","# 각 프레임을 나타내는 이미지 feature를 비디오 별로 평균 내어 비디오 feature로 사용\n","# 비디오 feature로 학습 후, 행동 예측\n","# hint : 위 셀에서 선언한 train_global_id, train_global_label, test_global_id 및 np.mean() 활용\n","###########################################################################\n","\n","train_avg_video_desc = []\n","train_avg_video_label = []\n","\n","for i in np.unique(train_global_id):\n","    tind = np.where(train_global_id == i)[0]\n","    train_avg_video_desc.append(np.mean(train_global_desc[tind], axis=0))\n","    train_avg_video_label.append(np.unique(train_global_label[tind]))\n","\n","train_avg_video_desc = np.asarray(train_avg_video_desc)\n","train_avg_video_label = np.asarray(train_avg_video_label).ravel()\n","\n","test_avg_video_desc = []\n","for i in np.unique(test_global_id):\n","    tind = np.where(test_global_id == i)[0]\n","    test_avg_video_desc.append(np.mean(test_global_desc[tind], axis=0))\n","test_avg_video_desc = np.asarray(test_avg_video_desc)\n","\n","svm = SVC(random_state=0)\n","svm.fit(train_avg_video_desc, train_avg_video_label)\n","svm_predict = svm.predict(test_avg_video_desc)\n","saveFile(classinfo_arr[svm_predict][:,1], np.arange(len(test_list)), \"svm_global_averaging\")\n","print(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))"]},{"cell_type":"markdown","metadata":{},"source":["# 비디오의 모든 프레임에서 얻은 feature로 분류기를 사용해 예측하고 예측치 중 가장 많은 빈도를 나타낸 행동을 선정 (Empty Module 6)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T15:57:43.477395Z","iopub.status.busy":"2022-05-09T15:57:43.477082Z","iopub.status.idle":"2022-05-09T15:58:43.633759Z","shell.execute_reply":"2022-05-09T15:58:43.63265Z","shell.execute_reply.started":"2022-05-09T15:57:43.477364Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","SVM global voting in frame\n","\t10.34s\n","\n","\n"]}],"source":["print(\"\\n\\nSVM global voting in frame\")\n","start = time.time()\n","############################### Empty Module 6 : SVM (voting) ##################################\n","# 제약조건 : sklearn의 SVC 사용 및 random_state=0으로 고정\n","#          베이스라인 파라미터는 defalut 값을 이용하였습니다\n","# 각 프레임을 나타내는 이미지 feature를 모두 사용해 SVM 학습\n","# 학습 시, 각 이미지 feature의 label은 해당되는 비디오의 label(행동)로 사용\n","# 프레임 별로 행동 예측 후, 같은 비디오 내 프레임의 행동 예측 값의 최빈값을 해당 비디오의 행동 예측 값으로 선정***\n","# hint : 위 셀에서 선언한 train_global_label, test_global_id 및 mode 활용\n","################################################################################################\n","\n","\n","\n","svm = SVC(random_state=0)\n","svm.fit(train_global_desc, train_global_label)\n","svm_predict = svm.predict(test_global_desc)\n","svm_predict_vote = []\n","for i in np.unique(test_global_id):\n","    tind = np.where(test_global_id == i)[0]\n","    voted = mode(svm_predict[tind]).mode.item()\n","    svm_predict_vote.append(voted)\n","svm_predict_vote = np.asarray(svm_predict_vote)\n","\n","saveFile(classinfo_arr[svm_predict_vote][:,1], np.arange(len(test_list)), \"svm_global_voting\")\n","print(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))"]},{"cell_type":"markdown","metadata":{},"source":["# 프레임 feature에서 대표되는 feature를 선정 후 BoW or VLAD 방식으로 비디오 feature를 기술 (Empty Module 7)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T15:58:43.636091Z","iopub.status.busy":"2022-05-09T15:58:43.635628Z","iopub.status.idle":"2022-05-09T15:58:45.270624Z","shell.execute_reply":"2022-05-09T15:58:45.269644Z","shell.execute_reply.started":"2022-05-09T15:58:43.636042Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["train_global_alloc, test_global_alloc, global_codebook, global_kmeans = clustering(train_global_desc, test_global_desc, args_global_cluster)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T15:58:45.276973Z","iopub.status.busy":"2022-05-09T15:58:45.276667Z","iopub.status.idle":"2022-05-09T15:58:57.758852Z","shell.execute_reply":"2022-05-09T15:58:57.757718Z","shell.execute_reply.started":"2022-05-09T15:58:45.276943Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","Allocate center & Descript global histogram\n","\t2.08s\n","\n","\n"]}],"source":["print(\"\\n\\nAllocate center & Descript global histogram\")\n","start = time.time()\n","train_vid_names = np.asarray([i.split(\", \")[0] for i in train_global_desc_key])\n","train_vid_names_u = np.unique(train_vid_names)\n","\n","# Train 비디오 내 프레임 별로 기술된 이미지 feature를 기반으로 한번 더 기술하여(한번 더 BoW 혹은 VLAD)\n","# 각 비디오에 대한 비디오 feature 기술\n","# Empty Module 7과 관련있으며, 5,6과는 무관\n","train_video_desc = []\n","train_video_desc_key = []\n","for vid_name in train_vid_names_u:\n","    cind = np.where(vid_name==train_vid_names)[0]\n","    if args_aggr==\"bow\":\n","        hist_desc = BoW(train_global_alloc[cind], args_global_cluster)\n","    elif args_aggr==\"vlad\":\n","        hist_desc = VLAD(train_global_desc[cind], train_global_alloc[cind], global_codebook)\n","    else:\n","        print(\"Set the appropriate variable args_aggr!!\")\n","        import pdb; pdb.set_trace()\n","\n","    train_video_desc.append(hist_desc)\n","    train_video_desc_key.append(vid_name)\n","train_video_desc = np.asarray(train_video_desc)\n","train_video_desc_key = np.asarray(train_video_desc_key)\n","\n","# Test 비디오 내 프레임 별로 기술된 이미지 feature를 기반으로 한번 더 기술하여(한번 더 BoW 혹은 VLAD)\n","# 각 비디오에 대한 비디오 feature 기술\n","# Empty Module 7과 관련있으며, 5,6과는 무관\n","test_vid_names = np.asarray([i.split(\", \")[0] for i in test_global_desc_key])\n","test_vid_names_u = np.unique(test_vid_names)\n","\n","test_video_desc = []\n","test_video_desc_key = []\n","for vid_name in test_vid_names_u:\n","    cind = np.where(vid_name==test_vid_names)[0]\n","    if args_aggr==\"bow\":\n","        hist_desc = BoW(test_global_alloc[cind], args_global_cluster)\n","    elif args_aggr==\"vlad\":\n","        hist_desc = VLAD(test_global_desc[cind], test_global_alloc[cind], global_codebook)\n","    else:\n","        print(\"Set the appropriate variable args_aggr!!\")\n","        import pdb; pdb.set_trace()\n","\n","    test_video_desc.append(hist_desc)\n","    test_video_desc_key.append(vid_name)\n","test_video_desc = np.asarray(test_video_desc)\n","test_video_desc_key = np.asarray(test_video_desc_key)\n","\n","\n","print(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T15:58:57.761065Z","iopub.status.busy":"2022-05-09T15:58:57.760441Z","iopub.status.idle":"2022-05-09T15:58:58.025788Z","shell.execute_reply":"2022-05-09T15:58:58.024685Z","shell.execute_reply.started":"2022-05-09T15:58:57.761002Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","Processing label\n","\t0.05s\n","\n","\n"]}],"source":["print(\"\\n\\nProcessing label\")\n","start = time.time()\n","\n","# 분류를 위해, 행동 분류에 대한 각 train 비디오 별 label 가공\n","train_video_id = np.array([int(i.split(\"/\")[-1].split(\".\")[0]) for i in train_video_desc_key])\n","train_video_label = []\n","for fid in train_video_id:\n","    cind = np.where(train_csv_arr[:, 0]==fid)[0]\n","    clsname = train_csv_arr[cind, 1]\n","    cinfo_ind = np.where(classinfo_arr[:, 1] == clsname)[0]\n","    train_video_label.append(classinfo_arr[cinfo_ind, 0].astype(\"int\"))\n","train_video_label = np.asarray(train_video_label).ravel()\n","\n","# 분류를 위해, 행동 분류에 대한 각 test 비디오 별 id 가공\n","test_video_id = np.array([int(i.split(\"/\")[-1].split(\".\")[0]) for i in test_video_desc_key])\n","\n","print(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T15:58:58.027846Z","iopub.status.busy":"2022-05-09T15:58:58.027235Z","iopub.status.idle":"2022-05-09T15:59:05.849777Z","shell.execute_reply":"2022-05-09T15:59:05.84864Z","shell.execute_reply.started":"2022-05-09T15:58:58.027798Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","Reduce dim of descriptor of the frames with PCA\n","\t4.65s\n","\n","\n"]}],"source":["# 이미지 feature에 대해 다시 한번 VLAD feature 기술 방식을 사용하여 video feature를 기술한 경우 큰 차원으로 인해 메모리 부족 현상이 발생하므로 PCA를 이용한 차원 축소\n","if args_aggr==\"vlad\":\n","    print(\"\\n\\nReduce dim of descriptor of the frames with PCA\")\n","    start = time.time()\n","    pca = PCA(n_components=pca_vlad, random_state=0)\n","    pca.fit(train_video_desc)\n","    train_video_desc = pca.transform(train_video_desc)\n","    test_video_desc = pca.transform(test_video_desc)\n","    print(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T15:59:05.852141Z","iopub.status.busy":"2022-05-09T15:59:05.851468Z","iopub.status.idle":"2022-05-09T15:59:07.452192Z","shell.execute_reply":"2022-05-09T15:59:07.451051Z","shell.execute_reply.started":"2022-05-09T15:59:05.852089Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","SVM video descriptor\n","\t0.50s\n","\n","\n"]}],"source":["print(\"\\n\\nSVM video descriptor\")\n","start = time.time()\n","######################## Empty Module 7 : SVM (video feature) ##########################\n","# 제약조건 : sklearn의 SVC 사용 및 random_state=0으로 고정\n","#          베이스라인 파라미터는 defalut 값을 이용하였습니다\n","# 이전 이미지 feature를 기반으로 각 기술방식(BoW, VLAD)을 한번 더 적용해 만든 비디오 feature 사용\n","# 비디오 feature로 학습 후, 행동 예측\n","#######################################################################################\n","\n","svm = SVC(random_state=0)\n","svm.fit(train_video_desc, train_video_label)\n","svm_predict = svm.predict(test_video_desc)\n","saveFile(classinfo_arr[svm_predict][:,1], test_video_id, \"svm_video\")\n","print(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))"]}],"metadata":{"interpreter":{"hash":"3b56a72e9acabfb7e93d1aa640bab3f729f4fa7900e427b4a015597563d3c661"},"kernelspec":{"display_name":"Python 3.9.7 64-bit ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":4}
