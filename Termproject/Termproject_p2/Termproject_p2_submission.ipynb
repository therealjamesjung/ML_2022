{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 반드시 처음부터 끝까지 스켈레톤 코드를 살펴보고 구현하기 시작하길 바란다\n","\n","## 1. 스켈레톤 코드를 [복사 및 편집] 하여 사용한다.\n","## 2. 아래의 [Empty Module 3개]를 직접 구현한다.\n","## (필수) 코드 작성 전에 Overview의 Description을 읽고, 본 프로젝트의 방향성을 이해하고 시작하세요."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T13:47:32.409711Z","iopub.status.busy":"2022-05-09T13:47:32.409396Z","iopub.status.idle":"2022-05-09T13:47:32.417766Z","shell.execute_reply":"2022-05-09T13:47:32.417154Z","shell.execute_reply.started":"2022-05-09T13:47:32.409678Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        os.path.join(dirname, filename)\n","        #print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T13:47:32.66145Z","iopub.status.busy":"2022-05-09T13:47:32.661036Z","iopub.status.idle":"2022-05-09T13:47:34.308556Z","shell.execute_reply":"2022-05-09T13:47:34.307534Z","shell.execute_reply.started":"2022-05-09T13:47:32.66142Z"},"trusted":true},"outputs":[],"source":["# 본 프로젝트를 위한 패키지 로드\n","import re\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords"]},{"cell_type":"markdown","metadata":{},"source":["# 자연어 처리를 위한 라이브러리 다운로드\n","- nltk 라이브러리에서 punkt 데이터를 다운 받음, 이 데이터는 영화 리뷰와 같은 문서 데이터로 문자의 tokeninizer를 위해서 필요하다\n","- nltk 라이브러리를 이용해서 불용어를 다운 받음"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T13:47:35.105797Z","iopub.status.busy":"2022-05-09T13:47:35.105539Z","iopub.status.idle":"2022-05-09T13:47:35.279568Z","shell.execute_reply":"2022-05-09T13:47:35.27862Z","shell.execute_reply.started":"2022-05-09T13:47:35.10577Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     /Users/therealjamesjung/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     /Users/therealjamesjung/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('punkt')\n","nltk.download('stopwords')"]},{"cell_type":"markdown","metadata":{},"source":["# 데이터 로드"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T13:48:33.084552Z","iopub.status.busy":"2022-05-09T13:48:33.083791Z","iopub.status.idle":"2022-05-09T13:48:33.108063Z","shell.execute_reply":"2022-05-09T13:48:33.10738Z","shell.execute_reply.started":"2022-05-09T13:48:33.084506Z"},"trusted":true},"outputs":[],"source":["df_train = pd.read_csv(\"train.csv\",encoding=\"latin-1\")\n","df_test = pd.read_csv(\"test.csv\",encoding=\"latin-1\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T13:49:19.838045Z","iopub.status.busy":"2022-05-09T13:49:19.837712Z","iopub.status.idle":"2022-05-09T13:49:19.848371Z","shell.execute_reply":"2022-05-09T13:49:19.847479Z","shell.execute_reply.started":"2022-05-09T13:49:19.838011Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Data</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>No I'm in the same boat. Still here at my moms...</td>\n","      <td>ham</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>(Bank of Granite issues Strong-Buy) EXPLOSIVE ...</td>\n","      <td>spam</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>They r giving a second chance to rahul dengra.</td>\n","      <td>ham</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>O i played smash bros  &amp;lt;#&amp;gt;  religiously.</td>\n","      <td>ham</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>PRIVATE! Your 2003 Account Statement for 07973...</td>\n","      <td>spam</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0                                               Data Class\n","0           0  No I'm in the same boat. Still here at my moms...   ham\n","1           1  (Bank of Granite issues Strong-Buy) EXPLOSIVE ...  spam\n","2           2     They r giving a second chance to rahul dengra.   ham\n","3           3     O i played smash bros  &lt;#&gt;  religiously.   ham\n","4           4  PRIVATE! Your 2003 Account Statement for 07973...  spam"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df_train.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["X_train = df_train[\"Data\"]\n","y_train = df_train[\"Class\"]\n","X_test = df_test[\"Data\"]"]},{"cell_type":"markdown","metadata":{},"source":["# [Empty Module #1] 텍스트 데이터 전처리  \n","\n","목표: 텍스트 데이터를 처리하기 위한 여러 과정을 거쳐, 머신을 위한 데이터를 만든다. \n","\n","```\n","[input]\n","--------------\n","- text: 텍스트 문장 데이터 \n","\n","[output]\n","--------------\n","- text: 전처리를 완료한 문장 데이터 \n","    \n","```"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[],"source":["# ------------------------------------------------\n","# [Empty Module #1] 텍스트 데이터 전처리\n","# ------------------------------------------------\n","from nltk.corpus import stopwords \n","from nltk.stem.porter import PorterStemmer\n","\n","def data_processing(text):\n","    # ------------------------------------------------------------\n","    # 구현 가이드라인 \n","    # ------------------------------------------------------------\n","    # [1] re.sub 사용해 text 속 '[^A-Za-z]' 외의 문자만을 찾아내 제거한후, pre_words 변수에 저장\n","    #      1) pattern은 '[^A-Za-z]', repl=' ' 로 각각 설정.\n","    #      2) 이모지나 숫자,점과 같은 문자외의 것들을 제거했다. (이모지는 감정 분석과 관련해서 몇가지 의미를 나타내지만 이 테스크에서는 이러한 의미도 제거함.)\n","    #\n","    # [2] pre_words의 lower 내장 함수를 이용해 대문자들은 소문자로 변경\n","    #      1)  대, 소문자가 구분되어 있으면 \"Go\"와 \"go\" 와 같이 동일한 단어를 머신은 다른 단어로 취급한다. 따라서 대문자를 모두 소문자로 변경.\n","    #\n","    # [3] word_tokenize 함수를 이용해 pre_word 를 토큰화하여 word를 리스트화한 후 tokenized_words변수에 저장\n","    #\n","    # [4] nltk 라이브러리로 다운 받은 stopwords의 \"words\" 내장 함수를 이용해 english 불용어를 찾아서 stops 변수에 저장  \n","    #      1) 불용어: 텍스트 분류에서 불용어는 텍스트의 중요도을 결정하는데 영향을 미치지 않는 단어임. \n","    #                    (ex: the, we, a , will), 따라서 불필요한 단어가 예측 모델에 악영향을 끼칠 수 있기 때문에 제거.\n","    #\n","    # [5] [3] 에서 찾은 문자열 중 단어가 [4] 에서 찾은 불용어 속에 없을 경우, tokenized_words_remove 리스트에 append \n","    #\n","    # [6] tokenized_words_remove의 단어를 PorterStemmer 속 stem 내장 함수를 이용해, 동일 의미를 갖는 단어를 동일한 단어로 변경하는 과정을 거친 후 다시 저장.\n","    #    \n","    # ------------------------------------------------------------\n","    ##############\n","    # [1]\n","    pre_words = re.sub('[^A-Za-z]', ' ', text) # 정규표현식을 이용해서 문자 이외의 것 제거\n","    ##############\n","    # [2]\n","    pre_words = pre_words.lower() # 문자열 소문자로 변경\n","    ##############\n","    # [3]\n","    tokenized_words = word_tokenize(pre_words) # pre_words 토큰화\n","    ##############\n","    # [4]\n","    stops = stopwords.words('english') # english 불용어 불러오기\n","    ##############\n","    tokenized_words_remove=[]\n","    for w in tokenized_words:\n","        # [5]\n","        if w not in stops:\n","            tokenized_words_remove.append(w) # 불용어가 아닌 문자들 tokenized_words_remove에 append\n","\n","    stemmer = PorterStemmer()\n","    for i in range(len(tokenized_words_remove)):\n","        tokenized_words_remove[i] = stemmer.stem(tokenized_words_remove[i]) # PorterStemmer 사용하여 동일한 의미의 단어들로 변경\n","        # [6]\n","    \n","    return ( \" \".join( tokenized_words_remove ))"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["X_train = [data_processing(i) for i in X_train]\n","X_test = [data_processing(i) for i in X_test]"]},{"cell_type":"markdown","metadata":{},"source":["# [Empty Module #2] Bag of Word \n","\n","목표: 문장 데이터를 머신을 학습하기 위한 실수화된 Feature로 변경하기로한다. \n","\n","- train 과 test 데이터 전부 type을 ('U')로 변경하여 Countvectorizer를 사용한다. \n","- [설명 링크](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html?highlight=countvectorizer#sklearn.feature_extraction.text.CountVectorizer)"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["# ------------------------------------------------\n","# [Empty Module #2] 텍스트 데이터 Bag of word  feature  화 \n","# ------------------------------------------------\n","from sklearn.feature_extraction.text import CountVectorizer\n","# ------------------------------------------------------------\n","# 구현 가이드라인 \n","# ------------------------------------------------------------\n","# [1] CountVectorizer를 정의\n","#           1) max_features를 100으로 지정 \n","#\n","# [2] X_train 과 X_test를 numpy array로 변환 후 데이터 타입을 \"U\"로 변경해 저장\n","#\n","# [3] CountVectorizer를 이용해 X_train은 학습 및 변환(fit_transform)을 하고, X_test는 변환(transform)을 진행. \n","# ------------------------------------------------------------\n","\n","##############\n","# [1]\n","vectorizer = CountVectorizer(max_features=100) # Vectorizer max_features 100으로 지정\n","##############\n","\n","# [2]\n","X_train, X_test = np.array(X_train, dtype='U'), np.array(X_test, dtype='U') # 데이터 타입 유니코드로 변경후 저장\n","\n","##############\n","# [3]\n","X_train_features = vectorizer.fit_transform(X_train) # train 데이터 벡터화\n","X_test_features = vectorizer.transform(X_test) # test 데이터 벡터화\n","##############"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/wr/bqxxn9ss13b7snt4sxqbl0zm0000gn/T/ipykernel_12365/3383662999.py:2: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  y_train[y_train==\"ham\"] = 0\n","/var/folders/wr/bqxxn9ss13b7snt4sxqbl0zm0000gn/T/ipykernel_12365/3383662999.py:3: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  y_train[y_train==\"spam\"] = 1\n"]}],"source":["# ham의 경우 0으로 지정하고, spam의 경우에는 1로 라벨을 변경해줌\n","y_train[y_train==\"ham\"] = 0\n","y_train[y_train==\"spam\"] = 1\n","y_train = y_train.astype(\"uint8\")"]},{"cell_type":"markdown","metadata":{},"source":["## [Empty Module #3] SVM: classifier\n","목표: SVC() 를 활용해 classification 을 진행\n","\n","- fit()으로 train 에 대한 머신러닝 학습\n","- predict()으로 test 에 대한 정답을 추론 하여 반환"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["# ------------------------------------------------\n","# [Empty Module #3] 텍스트 데이터 Bag of word  feature  화 \n","# ------------------------------------------------\n","from sklearn.svm import SVC\n","# ------------------------------------------------------------\n","# 구현 가이드라인 \n","# ------------------------------------------------------------\n","# [1]  SVC 선언 (베이스라인 에서 gamma=\"auto\" 사용 )\n","#\n","# [2] X_train_features과 y_train으로 SVC 학습진행 후, X_test_features로 predict 진행\n","#\n","# [3] y_pred에 predict한 결과값 저장\n","# ------------------------------------------------------------\n","###########\n","# [1]\n","svc = SVC()\n","##############\n","\n","# [2]\n","svc.fit(X_train_features, y_train) # SVC 모델 학습\n","\n","##############\n","# [3]\n","y_pred = svc.predict(X_test_features) # 결과값 저장\n","##############"]},{"cell_type":"markdown","metadata":{},"source":["# Predict to CSV"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[],"source":["df_pred={\"ID\": range(np.array(y_pred).shape[0]),\"Class\":y_pred}\n","df_pred=pd.DataFrame(df_pred)\n","df_pred.to_csv(\"predict.csv\",index=False)"]}],"metadata":{"interpreter":{"hash":"3b56a72e9acabfb7e93d1aa640bab3f729f4fa7900e427b4a015597563d3c661"},"kernelspec":{"display_name":"Python 3.9.7 64-bit ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":4}
